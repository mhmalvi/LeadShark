# Google Sheets Prospect Enrichment ü¶à

> **Production-grade CLI with Smart Column Detection & ToS-Compliant Data Collection**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A sophisticated prospect enrichment system that reads Google Sheets, discovers URLs automatically, scrapes/enriches data while respecting ToS/robots.txt, and writes results back with intelligent column management.

## ‚ú® Key Features

- üß† **Smart Column Detection** - Automatically creates and manages enrichment columns
- üåê **Multi-Platform Support** - Website, Twitter/X, YouTube, GitHub, News articles
- üõ°Ô∏è **ToS Compliance** - Respects robots.txt and platform terms of service
- üéØ **Deterministic Scoring** - Weighted rubric for consistent lead qualification
- ‚ö° **Rate Limiting** - Per-domain throttling with intelligent retry logic
- üíæ **Smart Caching** - Reduces redundant requests with TTL-based caching
- üîÑ **Idempotent Operations** - Re-runs safely overwrite existing data
- üìä **Rich CLI Interface** - Beautiful progress tracking and error reporting

## üèóÔ∏è Architecture

### Core Components

```
app.py                          # CLI entrypoint with argument parsing
‚îú‚îÄ‚îÄ handlers/                   # URL-specific processors
‚îÇ   ‚îú‚îÄ‚îÄ website.py             # Generic website scraping
‚îÇ   ‚îú‚îÄ‚îÄ twitter.py             # Twitter/X API integration
‚îÇ   ‚îú‚îÄ‚îÄ youtube.py             # YouTube Data API
‚îÇ   ‚îú‚îÄ‚îÄ github.py              # GitHub API integration
‚îÇ   ‚îú‚îÄ‚îÄ news.py                # News article processing
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py        # Coordination and workflow
‚îî‚îÄ‚îÄ utils/                      # Core utilities
    ‚îú‚îÄ‚îÄ sheets.py              # Google Sheets management
    ‚îú‚îÄ‚îÄ normalize.py           # URL extraction and normalization
    ‚îú‚îÄ‚îÄ robots.py              # Robots.txt compliance checking
    ‚îú‚îÄ‚îÄ scoring.py             # Deterministic lead scoring
    ‚îú‚îÄ‚îÄ cache.py               # File-based caching system
    ‚îî‚îÄ‚îÄ logging.py             # Structured logging setup
```

### Smart Column Management

The system automatically detects and manages enrichment columns with a consistent namespace:

- `ENRICH_LINK_1_SUMMARY` through `ENRICH_LINK_N_SUMMARY` - Per-link summaries
- `ENRICH_COMBINED_REPORT` - Comprehensive analysis
- `ENRICH_LEAD_SCORE` - Numerical score (0-100)
- `ENRICH_LEAD_SCORE_NOTES` - Scoring rationale
- `ENRICH_STATUS` - Processing status
- `ENRICH_LAST_RUN` - Timestamp

## üöÄ Quick Start

### 1. Installation

```bash
git clone <repository-url>
cd LeadShark
pip install -r requirements.txt
```

### 2. Google Sheets API Setup

**Option A: OAuth (Recommended for personal use)**

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select existing
3. Enable the Google Sheets API
4. Create OAuth 2.0 credentials
5. Download `credentials.json` to project root

**Option B: Service Account (For automation)**

1. Create a service account in Google Cloud Console
2. Generate and download JSON key file
3. Share your Google Sheet with the service account email

### 3. Configuration

```bash
cp .env.example .env
# Edit .env with your configuration
```

Essential settings:
```env
GOOGLE_SHEET_ID=your_sheet_id_here
GOOGLE_AUTH_MODE=oauth
WORKSHEET_NAME=Sheet1
```

### 4. First Run

```bash
# Test with dry run
python app.py --sheet-id YOUR_SHEET_ID --dry-run

# Process first 10 rows
python app.py --sheet-id YOUR_SHEET_ID --rows "2-11"

# Full processing
python app.py --sheet-id YOUR_SHEET_ID
```

## üìä Expected Data Format

Your Google Sheet should contain columns with prospect information. The system automatically discovers URL columns by:

### Automatic URL Discovery

**Header Patterns** (case-insensitive):
- `*link*`, `*url*`, `*website*`, `*site*`
- `*twitter*`, `*x.com*`, `*youtube*`, `*github*`
- `*social*`, `*profile*`, `*portfolio*`, `*company*`

**Content Detection**:
- Cells containing URLs (http/https)
- Domain-like text (example.com)

### Sample Sheet Structure

| name     | company    | website           | social_links                              |
|----------|------------|-------------------|-------------------------------------------|
| John Doe | TechCorp   | https://tech.com  | https://twitter.com/john, github.com/john |
| Jane S.  | StartupCo  | startupco.com     | https://linkedin.com/in/jane              |

## üéØ Scoring System

The deterministic scoring system uses a weighted rubric (totaling 100%):

- **Relevance to Services (30%)** - Technology keywords, industry alignment
- **Purchase Intent (25%)** - Pricing pages, "contact sales", hiring signals
- **Authority/Size (20%)** - Follower counts, GitHub stars, company indicators
- **Recency (15%)** - Recent posts, updates, activity within 90 days
- **Data Quality (10%)** - Success rate, source diversity, content richness

### Score Ranges
- **90-100**: Premium prospects (strong intent + authority)
- **70-89**: High-value prospects (good fit with some intent signals)
- **50-69**: Qualified prospects (relevant but lower intent)
- **30-49**: Potential prospects (basic relevance)
- **0-29**: Low-priority prospects (poor fit or insufficient data)

## üõ°Ô∏è Compliance & Ethics

### Strict ToS Compliance

- **LinkedIn**: Never scraped (ToS violation) - automatically skipped
- **Robots.txt**: Checked and respected for all websites
- **Rate Limiting**: Configurable per-domain limits (default: 0.2 RPS)
- **API Preference**: Uses official APIs when tokens provided

### Forbidden Domains

The system maintains a list of explicitly forbidden domains that are never scraped:
- `linkedin.com` - ToS explicitly prohibits scraping
- Additional domains can be configured

### Respectful Scraping

- User-Agent identification with contact information
- Exponential backoff on rate limit responses
- Per-domain concurrency limiting
- Graceful error handling and recovery

## üìã CLI Usage

### Basic Commands

```bash
# Process entire sheet
python app.py --sheet-id 1ABC123... --worksheet "Sheet1"

# Process specific row range
python app.py --sheet-id 1ABC123... --rows "2-100"

# Dry run (preview without writing)
python app.py --sheet-id 1ABC123... --dry-run

# Process only new rows (empty COMBINED_REPORT)
python app.py --sheet-id 1ABC123... --only-new

# Restrict to specific domains
python app.py --sheet-id 1ABC123... --force-domains "twitter.com,github.com"

# Custom number of link summaries
python app.py --sheet-id 1ABC123... --max-link-summaries 3

# Verbose logging
python app.py --sheet-id 1ABC123... --verbose -vv
```

### Advanced Options

```bash
# Bypass cache
python app.py --sheet-id 1ABC123... --no-cache

# Custom worksheet
python app.py --sheet-id 1ABC123... --worksheet "Prospects Q1"

# Process subset with custom summaries
python app.py --sheet-id 1ABC123... --rows "10-50" --max-link-summaries 2 --dry-run
```

## üîß Configuration

### Environment Variables

```env
# Google Sheets
GOOGLE_AUTH_MODE=oauth|service_account
GOOGLE_SHEET_ID=your_sheet_id
WORKSHEET_NAME=Sheet1

# Processing
INPUT_COLUMNS=auto
MAX_LINK_SUMMARIES=5
HEADER_NAMESPACE=ENRICH_

# Rate Limiting
PER_DOMAIN_RPS=0.2
TIMEOUT_SECONDS=20
USER_AGENT=ProspectResearchBot/1.0 (+contact@example.com)

# API Keys (Optional)
TWITTER_BEARER=your_twitter_bearer_token
YOUTUBE_API_KEY=your_youtube_api_key
GITHUB_TOKEN=your_github_token

# Caching
CACHE_TTL=86400
CACHE_DIR=./.cache
```

### API Key Setup

**Twitter/X API** (Improves data quality):
1. Apply at [developer.twitter.com](https://developer.twitter.com/)
2. Create app and generate Bearer Token
3. Add to `TWITTER_BEARER` in .env

**YouTube Data API**:
1. Enable in Google Cloud Console
2. Create API key
3. Add to `YOUTUBE_API_KEY` in .env

**GitHub Token** (Higher rate limits):
1. Generate at [github.com/settings/tokens](https://github.com/settings/tokens)
2. Select public repo read permissions
3. Add to `GITHUB_TOKEN` in .env

## üß™ Testing

Run the comprehensive test suite:

```bash
# All tests
pytest

# Specific test categories
pytest tests/test_url_extraction.py      # URL handling
pytest tests/test_column_management.py   # Smart columns
pytest tests/test_scoring_determinism.py # Scoring consistency
pytest tests/test_integration.py         # End-to-end scenarios

# With coverage
pytest --cov=handlers --cov=utils

# Verbose output
pytest -v -s
```

### Test Scenarios

The test suite covers all specification requirements:

- **URL Extraction**: Multi-URL cells, deduplication, normalization
- **Column Management**: Detect existing blocks, create missing headers, expand/contract
- **Scoring Determinism**: Consistent scores for identical inputs
- **Integration**: Complete workflows with mock data

## üìà Performance & Scalability

### Processing Speed
- **60-120 prospects/hour** (depending on URL count and API availability)
- **Concurrent domain processing** with rate limiting
- **Intelligent caching** reduces redundant requests by 70-80%

### Memory Usage
- **Minimal memory footprint** with streaming processing
- **File-based caching** with automatic cleanup
- **Batch operations** for Google Sheets API efficiency

### Monitoring
- **Rich progress bars** with real-time status
- **Detailed logging** with configurable levels
- **Error categorization** and retry statistics

## üö® Troubleshooting

### Common Issues

**Authentication Errors**:
```bash
# Regenerate OAuth token
rm token.json
python app.py --sheet-id YOUR_ID --dry-run
```

**Rate Limiting (429 errors)**:
```env
# Reduce rate in .env
PER_DOMAIN_RPS=0.1
```

**Missing API Data**:
```env
# Add API tokens for better coverage
TWITTER_BEARER=your_token
YOUTUBE_API_KEY=your_key
```

**Sheet Permission Errors**:
- Ensure sheet is shared with OAuth account or service account email
- Check GOOGLE_SHEET_ID is correct (from sheet URL)

### Debug Mode

```bash
# Maximum verbosity
python app.py --sheet-id YOUR_ID --verbose -vvv

# Check cache statistics
python -c "from utils.cache import CacheManager; print(CacheManager().get_cache_stats())"

# Validate configuration
python -c "from app import validate_environment; print(validate_environment())"
```

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`pytest`)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open Pull Request

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements.txt
pip install pytest-asyncio pytest-cov

# Run pre-commit checks
pytest
flake8 handlers/ utils/ app.py
```

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üôã Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Documentation**: See `docs/` directory
- **Examples**: Check `examples/` for sample configurations

## üö® Disclaimer

This tool processes publicly available information only. Users are responsible for:
- Compliance with applicable laws and regulations
- Respecting platform Terms of Service
- Maintaining data privacy and security
- Ethical use of collected information

Always review and respect robots.txt files and rate limits. The tool is designed to be respectful and compliant, but ultimate responsibility lies with the user.

---

**ü¶à Built for predatory precision in prospect enrichment while maintaining ethical standards**